{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import cv2\n",
    "\n",
    "from attribute_predictor import AttributePredictor\n",
    "from attribute_predictor_SEBlocks import AttributePredictor_SEB\n",
    "from gradcam.utils import visualize_cam\n",
    "\n",
    "\n",
    "# Load and prepare the model\n",
    "def get_image_encoder(pretrained=True):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    \n",
    "    # Infer the output size of the image encoder\n",
    "    with torch.inference_mode():\n",
    "        out = model(torch.randn(5, 3, 224, 224))\n",
    "    assert out.dim() == 2\n",
    "    assert out.size(0) == 5\n",
    "    image_encoder_output_dim = out.size(1)\n",
    "    \n",
    "    return model, image_encoder_output_dim\n",
    "\n",
    "class GradCAMWrapper(torch.nn.Module):\n",
    "    def __init__(self, model, output_index=0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.output_index = output_index\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)[self.output_index]\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device)\n",
    "    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device)\n",
    "    return tensor * std[:, None, None] + mean[:, None, None]\n",
    "\n",
    "def load_model(checkpoint_path):\n",
    "    image_encoder, image_encoder_output_dim = get_image_encoder(pretrained=True)\n",
    "    attribute_sizes = [6]\n",
    "    \n",
    "    if checkpoint_path == \"./log/best_model_SEB1.pth\":\n",
    "        model = AttributePredictor_SEB(attribute_sizes,image_encoder_output_dim, image_encoder)\n",
    "    else:\n",
    "        model = AttributePredictor(attribute_sizes, image_encoder_output_dim, image_encoder)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def image_preprocessing(original_image, checkpoint_name):\n",
    "    \n",
    "    if checkpoint_name == \"Segmented nucleus model\":\n",
    "        #convert PIL image to opencv format\n",
    "        image = np.array(original_image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Convert to HSV color space and split channels\n",
    "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv_image)\n",
    "\n",
    "        # Split RGB channels\n",
    "        b, g, r = cv2.split(image)\n",
    "\n",
    "        # Subtract the S channel with the G channel\n",
    "        subtracted_image = cv2.subtract(s, g)\n",
    "        \n",
    "        # Threshold the subtracted image\n",
    "        _, thresh = cv2.threshold(subtracted_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Dilate the thresholded image \n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        dilated_thresh = cv2.dilate(thresh, kernel, iterations = 1)\n",
    "\n",
    "        # Convert the binary threshold image to 3 channels\n",
    "        thresh_3_channel = cv2.merge([dilated_thresh, dilated_thresh, dilated_thresh])\n",
    "\n",
    "        # Element-wise multiplication of the binary threshold with the original image\n",
    "        segmented_image = cv2.multiply(image, thresh_3_channel, scale=1/255)\n",
    "\n",
    "        # convert to BGR format\n",
    "        segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB)\n",
    "        # convert to PIL format\n",
    "        processed_img = Image.fromarray(segmented_image)\n",
    "\n",
    "        return processed_img\n",
    "    \n",
    "    elif checkpoint_name == \"Nucleus crop model\":\n",
    "\n",
    "        min_size=(150, 150)\n",
    "        #convert PIL image to opencv format\n",
    "        image = np.array(original_image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "        # Convert to image to HSV color space and split the channels\n",
    "        HSV_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        H, S, V = cv2.split(HSV_image)\n",
    "\n",
    "        # Split BGR channels \n",
    "        B, G, R = cv2.split(image)\n",
    "\n",
    "        # Subtract the S channel with the G channel\n",
    "        subtracted_image = cv2.subtract(S, G)\n",
    "        \n",
    "        # Threshold the subtracted image\n",
    "        ret, thresholded_image = cv2.threshold(subtracted_image, 0, 255, cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Dilate the thresholded image to improve contour detection\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        dilated_threshold_image = cv2.dilate(thresholded_image, kernel, iterations = 1)\n",
    "\n",
    "        # Find contours\n",
    "        contours, hierarchy = cv2.findContours(dilated_threshold_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Assuming the largest contour is the nucleus, if not empty\n",
    "        if contours:\n",
    "    \n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            original_x, original_y, original_w, original_h = cv2.boundingRect(largest_contour)\n",
    "            \n",
    "            # Calculate the center of the original bounding box\n",
    "            original_center_x = original_x + original_w // 2\n",
    "            original_center_y = original_y + original_h // 2\n",
    "\n",
    "            # Enforce minimum size, ensuring it's centered around the original bounding box\n",
    "            w = max(original_w, min_size[0])\n",
    "            h = max(original_h, min_size[1])\n",
    "\n",
    "            # Adjust x and y to crop the image around the center of the bounding box\n",
    "            new_x = max(original_center_x - w // 2, 0)\n",
    "            new_y = max(original_center_y - h // 2, 0)\n",
    "\n",
    "            # Adjust the end points, making sure we don't go out of the image boundaries\n",
    "            new_x_end = min(new_x + w, image.shape[1])\n",
    "            new_y_end = min(new_y + h, image.shape[0])\n",
    "\n",
    "            # Correct the coordinates if they go out of bounds\n",
    "            if new_x_end > image.shape[1]:\n",
    "                new_x = image.shape[1] - w\n",
    "            if new_y_end > image.shape[0]:\n",
    "                new_y = image.shape[0] - h\n",
    "\n",
    "            # Crop the image with the adjusted coordinates\n",
    "            cropped_nucleus = image[new_y:new_y_end, new_x:new_x_end]\n",
    "        \n",
    "            # convert back to RGB format for conversion back to PIL\n",
    "            cropped_nucleus = cv2.cvtColor(cropped_nucleus, cv2.COLOR_BGR2RGB)\n",
    "            # convert to PIL format\n",
    "            processed_image = Image.fromarray(cropped_nucleus)\n",
    "        return processed_image\n",
    "    \n",
    "    else:\n",
    "        return original_image\n",
    "    \n",
    "\n",
    "\n",
    "# Gradio function to handle image input, model prediction, and visualization\n",
    "def predict_and_visualize(original_image, checkpoint_name):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    # Transform the input image to match the model's expected input\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    original_image = image_preprocessing(original_image, checkpoint_name)\n",
    "    img = transform(original_image).unsqueeze(0).to(device)\n",
    "    checkpoint_path = checkpoint_path_all[checkpoint_name]  \n",
    "    model = load_model(checkpoint_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Prediction and Probability Calculation\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img)\n",
    "        \n",
    "    attribute_names = [\"nucleus_shape\"]\n",
    "    attribute_values = [\n",
    "        [\"irregular\", \"segmented-bilobed\", \"segmented-multilobed\", \"unsegmented-band\", \"unsegmented-indented\", \"unsegmented-round\"]\n",
    "    ]\n",
    "\n",
    "    # Collect all predictions\n",
    "    prediction_texts = []\n",
    "    \n",
    "    for i, logits in enumerate(predictions):\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        predicted_index = torch.argmax(probabilities, dim=1)\n",
    "        predicted_label = attribute_values[i][predicted_index.item()]\n",
    "        all_probabilities = probabilities.squeeze().tolist()\n",
    "        \n",
    "        print(f\"Predictions for {attribute_names[i]}:\")\n",
    "        for class_index, class_probability in enumerate(all_probabilities):\n",
    "            print(f\"{attribute_values[i][class_index]}: {class_probability*100:.2f}%\")\n",
    "        print(f\"Most likely: {predicted_label}, Probability: {all_probabilities[predicted_index.item()]*100:.2f}%\\n\")\n",
    "\n",
    "    # Grad-CAM setup\n",
    "    target_layer = model.image_encoder.layer4[-1]\n",
    "    gradcam_model_wrapper = GradCAMWrapper(model, output_index=0)\n",
    "    gradcam = GradCAM(gradcam_model_wrapper, target_layer)\n",
    "    gradcam_pp = GradCAMpp(gradcam_model_wrapper, target_layer)\n",
    "\n",
    "    mask, _ = gradcam(img)\n",
    "    heatmap, result = visualize_cam(mask, img)\n",
    "    if heatmap.ndim == 3 and heatmap.shape[0] == 3:\n",
    "        heatmap = heatmap[0]\n",
    "        \n",
    "    img_denorm = denormalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "\n",
    "    heatmap_norm = (heatmap.squeeze().cpu() - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "    colored_heatmap = plt.cm.jet(heatmap_norm.numpy())  # This applies the 'jet' colormap\n",
    "\n",
    "    # Convert colored heatmap to an image (discard the alpha channel)\n",
    "    heatmap_img = Image.fromarray((colored_heatmap[:, :, :3] * 255).astype(np.uint8))\n",
    "\n",
    "    # Combine the heatmap with the original image\n",
    "    img_pil = to_pil_image(img_denorm.squeeze()).convert(\"RGB\")\n",
    "    heatmap_on_image = Image.blend(img_pil, heatmap_img, alpha=0.4)\n",
    "    \n",
    "    # Combine the predictions into one string\n",
    "    combined_predictions = \"\\n\".join(prediction_texts)\n",
    "\n",
    "    return original_image, heatmap_img, heatmap_on_image, combined_predictions \n",
    "\n",
    "checkpoint_path_all = {\n",
    "    \"Default model\" : \"./log/best_model_nucleus.pth\",\n",
    "    \"Nucleus crop model\" : \"./log/best_model_nucleus_crop.pth\",\n",
    "    \"Segmented nucleus model\" : \"./log/best_model_segmented.pth\",\n",
    "    \"Squeeze-And-Excitation model\" : \"./log/best_model_SEB1.pth\"\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "iface = gr.Interface(fn=predict_and_visualize,\n",
    "                         inputs=[\n",
    "                            gr.Image(type=\"pil\"),\n",
    "                            gr.Dropdown(choices=checkpoint_path_all, label=\"Select Model\")\n",
    "                        ],\n",
    "                     outputs=[gr.Image(type=\"pil\", label=\"Original Image\"),\n",
    "                              gr.Image(type=\"pil\", label=\"Heatmap\"),\n",
    "                              gr.Image(type=\"pil\", label=\"Result on Image\"),\n",
    "                              gr.Textbox(label=\"Predictions\")],\n",
    "                     title=\"Attribute Prediction with Grad-CAM Visualization\",  \n",
    "                     description=\"Upload an image to predict attributes and visualize the model's focus areas.\",)                    \n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\functional.py:3769: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "from attribute_predictor import AttributePredictor\n",
    "from attribute_predictor_SEBlocks import AttributePredictor_SEB\n",
    "from gradcam.utils import visualize_cam\n",
    "\n",
    "\n",
    "# Load and prepare the model\n",
    "def get_image_encoder(pretrained=True):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    \n",
    "    # Infer the output size of the image encoder\n",
    "    with torch.inference_mode():\n",
    "        out = model(torch.randn(5, 3, 224, 224))\n",
    "    assert out.dim() == 2\n",
    "    assert out.size(0) == 5\n",
    "    image_encoder_output_dim = out.size(1)\n",
    "    \n",
    "    return model, image_encoder_output_dim\n",
    "\n",
    "class GradCAMWrapper(torch.nn.Module):\n",
    "    def __init__(self, model, output_index=0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.output_index = output_index\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)[self.output_index]\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device)\n",
    "    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device)\n",
    "    return tensor * std[:, None, None] + mean[:, None, None]\n",
    "\n",
    "def load_model(checkpoint_path):\n",
    "    image_encoder, image_encoder_output_dim = get_image_encoder(pretrained=True)\n",
    "    attribute_sizes = [6]\n",
    "    \n",
    "    if checkpoint_path == \"./log/best_model_SEB.pth\":\n",
    "        model = AttributePredictor_SEB(attribute_sizes,image_encoder_output_dim, image_encoder)\n",
    "    else:\n",
    "        model = AttributePredictor(attribute_sizes, image_encoder_output_dim, image_encoder)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def image_preprocessing(original_image, checkpoint_name):\n",
    "    \n",
    "    if checkpoint_name == \"Segmented nucleus model\":\n",
    "        #convert PIL image to opencv format\n",
    "        image = np.array(original_image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Convert to HSV color space and split channels\n",
    "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv_image)\n",
    "\n",
    "        # Split RGB channels\n",
    "        b, g, r = cv2.split(image)\n",
    "\n",
    "        # Subtract the S channel with the G channel\n",
    "        subtracted_image = cv2.subtract(s, g)\n",
    "        \n",
    "        # Threshold the subtracted image\n",
    "        _, thresh = cv2.threshold(subtracted_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Dilate the thresholded image \n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        dilated_thresh = cv2.dilate(thresh, kernel, iterations = 1)\n",
    "\n",
    "        # Convert the binary threshold image to 3 channels\n",
    "        thresh_3_channel = cv2.merge([dilated_thresh, dilated_thresh, dilated_thresh])\n",
    "\n",
    "        # Element-wise multiplication of the binary threshold with the original image\n",
    "        segmented_image = cv2.multiply(image, thresh_3_channel, scale=1/255)\n",
    "\n",
    "        # convert to BGR format\n",
    "        segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB)\n",
    "        # convert to PIL format\n",
    "        processed_img = Image.fromarray(segmented_image)\n",
    "\n",
    "        return processed_img\n",
    "    \n",
    "    elif checkpoint_name == \"Nucleus crop model\":\n",
    "\n",
    "        min_size=(150, 150)\n",
    "        #convert PIL image to opencv format\n",
    "        image = np.array(original_image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "        # Convert to image to HSV color space and split the channels\n",
    "        HSV_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        H, S, V = cv2.split(HSV_image)\n",
    "\n",
    "        # Split BGR channels \n",
    "        B, G, R = cv2.split(image)\n",
    "\n",
    "        # Subtract the S channel with the G channel\n",
    "        subtracted_image = cv2.subtract(S, G)\n",
    "        \n",
    "        # Threshold the subtracted image\n",
    "        ret, thresholded_image = cv2.threshold(subtracted_image, 0, 255, cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Dilate the thresholded image to improve contour detection\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        dilated_threshold_image = cv2.dilate(thresholded_image, kernel, iterations = 1)\n",
    "\n",
    "        # Find contours\n",
    "        contours, hierarchy = cv2.findContours(dilated_threshold_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Assuming the largest contour is the nucleus, if not empty\n",
    "        if contours:\n",
    "    \n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            original_x, original_y, original_w, original_h = cv2.boundingRect(largest_contour)\n",
    "            \n",
    "            # Calculate the center of the original bounding box\n",
    "            original_center_x = original_x + original_w // 2\n",
    "            original_center_y = original_y + original_h // 2\n",
    "\n",
    "            # Enforce minimum size, ensuring it's centered around the original bounding box\n",
    "            w = max(original_w, min_size[0])\n",
    "            h = max(original_h, min_size[1])\n",
    "\n",
    "            # Adjust x and y to crop the image around the center of the bounding box\n",
    "            new_x = max(original_center_x - w // 2, 0)\n",
    "            new_y = max(original_center_y - h // 2, 0)\n",
    "\n",
    "            # Adjust the end points, making sure we don't go out of the image boundaries\n",
    "            new_x_end = min(new_x + w, image.shape[1])\n",
    "            new_y_end = min(new_y + h, image.shape[0])\n",
    "\n",
    "            # Correct the coordinates if they go out of bounds\n",
    "            if new_x_end > image.shape[1]:\n",
    "                new_x = image.shape[1] - w\n",
    "            if new_y_end > image.shape[0]:\n",
    "                new_y = image.shape[0] - h\n",
    "\n",
    "            # Crop the image with the adjusted coordinates\n",
    "            cropped_nucleus = image[new_y:new_y_end, new_x:new_x_end]\n",
    "        \n",
    "            # convert back to RGB format for conversion back to PIL\n",
    "            cropped_nucleus = cv2.cvtColor(cropped_nucleus, cv2.COLOR_BGR2RGB)\n",
    "            # convert to PIL format\n",
    "            processed_image = Image.fromarray(cropped_nucleus)\n",
    "        return processed_image\n",
    "    \n",
    "    else:\n",
    "        return original_image\n",
    "    \n",
    "\n",
    "\n",
    "# Gradio function to handle image input, model prediction, and visualization\n",
    "def predict_and_visualize(original_image, checkpoint_name):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    obj = None\n",
    "    gc.collect()\n",
    "    \n",
    "    # Transform the input image to match the model's expected input\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    original_image = image_preprocessing(original_image, checkpoint_name)\n",
    "    img = transform(original_image).unsqueeze(0).to(device)\n",
    "    checkpoint_path = checkpoint_path_all[checkpoint_name]  \n",
    "    model = load_model(checkpoint_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img)\n",
    "\n",
    "    # Handle output if it's a list\n",
    "    if isinstance(predictions, list):\n",
    "        # Assuming the logits are the first element of the list\n",
    "        logits = predictions[0]\n",
    "    else:\n",
    "        logits = predictions\n",
    "\n",
    "    probabilities = F.softmax(logits.squeeze(), dim=0)\n",
    "    probabilities = probabilities.cpu().numpy()\n",
    "\n",
    "    attribute_values = [\"irregular\", \"segmented-bilobed\", \"segmented-multilobed\", \"unsegmented-band\", \"unsegmented-indented\", \"unsegmented-round\"]\n",
    "    results = {attribute_values[i]: float(probabilities[i]) for i in range(len(attribute_values))}\n",
    "\n",
    "\n",
    "    # Grad-CAM setup\n",
    "    target_layer = model.image_encoder.layer4[-1]\n",
    "    gradcam_model_wrapper = GradCAMWrapper(model, output_index=0)\n",
    "    gradcam = GradCAM(gradcam_model_wrapper, target_layer)\n",
    "    gradcam_pp = GradCAMpp(gradcam_model_wrapper, target_layer)\n",
    "\n",
    "    mask, _ = gradcam(img)\n",
    "    heatmap, result = visualize_cam(mask, img)\n",
    "    if heatmap.ndim == 3 and heatmap.shape[0] == 3:\n",
    "        heatmap = heatmap[0]\n",
    "        \n",
    "    img_denorm = denormalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "\n",
    "    heatmap_norm = (heatmap.squeeze().cpu() - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "    colored_heatmap = plt.cm.jet(heatmap_norm.numpy())  # This applies the 'jet' colormap\n",
    "\n",
    "    # Convert colored heatmap to an image (discard the alpha channel)\n",
    "    heatmap_img = Image.fromarray((colored_heatmap[:, :, :3] * 255).astype(np.uint8))\n",
    "\n",
    "    # Combine the heatmap with the original image\n",
    "    img_pil = to_pil_image(img_denorm.squeeze()).convert(\"RGB\")\n",
    "    heatmap_on_image = Image.blend(img_pil, heatmap_img, alpha=0.4)\n",
    "\n",
    "    return original_image, heatmap_on_image, results \n",
    "    # return original_image, heatmap_img, heatmap_on_image, results \n",
    "\n",
    "checkpoint_path_all = {\n",
    "    \"Default model\" : \"./log/best_model_nucleus.pth\",\n",
    "    \"Nucleus crop model\" : \"./log/best_model_nucleus_crop.pth\",\n",
    "    \"Segmented nucleus model\" : \"./log/best_model_segmented.pth\",\n",
    "    \"Squeeze-And-Excitation model\" : \"./log/best_model_SEB.pth\"\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "iface = gr.Interface(fn=predict_and_visualize,\n",
    "                         inputs=[\n",
    "                            gr.Image(type=\"pil\"),\n",
    "                            gr.Dropdown(choices=checkpoint_path_all, label=\"Select Model\")\n",
    "                        ],\n",
    "                     outputs=[gr.Image(type=\"pil\", label=\"Input Image\", width=224, height=224),\n",
    "                            #   gr.Image(type=\"pil\", label=\"Heatmap\",width=224, height=224),\n",
    "                              gr.Image(type=\"pil\", label=\"Result on Image\", width=224, height=224),\n",
    "                              gr.Label(num_top_classes=6, label=\"Prediction\"), ],\n",
    "                     title=\"Attribute Prediction with Grad-CAM Visualization\",  \n",
    "                     description=\"Upload an image to predict attributes and visualize the model's focus areas.\",)                    \n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
