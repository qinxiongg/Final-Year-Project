{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\functional.py:3769: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\PIL\\Image.py\", line 3070, in fromarray\n",
      "    mode, rawmode = _fromarray_typemap[typekey]\n",
      "                    ~~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
      "KeyError: ((1, 1, 224), '|u1')\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gradio\\blocks.py\", line 1786, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gradio\\blocks.py\", line 1338, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanqi\\AppData\\Local\\Temp\\ipykernel_18512\\1015675450.py\", line 84, in predict\n",
      "    heatmap_image = Image.fromarray((heatmap * 255).astype('uint8'))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\PIL\\Image.py\", line 3073, in fromarray\n",
      "    raise TypeError(msg) from e\n",
      "TypeError: Cannot handle this data type: (1, 1, 224), |u1\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Custom imports\n",
    "from attribute_predictor import AttributePredictor\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "from gradcam.utils import visualize_cam\n",
    "\n",
    "def get_image_encoder(pretrained=True):\n",
    "    model = models.resnet50(pretrained=pretrained)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = torch.nn.Identity()\n",
    "    return model, in_features\n",
    "\n",
    "image_encoder, image_encoder_output_dim = get_image_encoder(pretrained=True)\n",
    "attribute_sizes = [6]\n",
    "model = AttributePredictor(attribute_sizes, image_encoder_output_dim, image_encoder)\n",
    "checkpoint = torch.load('./log/best_model_nucleus.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "attribute_names = [\"nucleus_shape\"]\n",
    "attribute_values = [\n",
    "    [\"irregular\", \"segmented-bilobed\", \"segmented-multilobed\", \"unsegmented-band\", \"unsegmented-indented\", \"unsegmented-round\"]\n",
    "]\n",
    "\n",
    "class GradCAMWrapper(torch.nn.Module):\n",
    "    def __init__(self, model, output_index=0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.output_index = output_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[self.output_index]\n",
    "\n",
    "def predict(image):\n",
    "    image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)\n",
    "    \n",
    "    probabilities = [F.softmax(logits, dim=1) for logits in predictions]\n",
    "    results = {}\n",
    "    \n",
    "    for i, (probs, name) in enumerate(zip(probabilities, attribute_names)):\n",
    "        class_probs = probs.squeeze().tolist()\n",
    "        predicted_index = torch.argmax(probs)\n",
    "        results[name] = f\"{attribute_values[i][predicted_index.item()]} ({class_probs[predicted_index.item()]*100:.2f}%)\"\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.bar(attribute_values[i], class_probs)\n",
    "        ax.set_title(f\"Probabilities for {name}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"temp_plot_{i}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        results[f\"chart_{i}\"] = f\"temp_plot_{i}.png\"\n",
    "    \n",
    "    # Grad-CAM integration\n",
    "    target_layer = model.image_encoder.layer4[-1]\n",
    "    gradcam_model_wrapper = GradCAMWrapper(model, output_index=0)\n",
    "    gradcam = GradCAM(gradcam_model_wrapper, target_layer)\n",
    "    mask, _ = gradcam(img_tensor)\n",
    "    heatmap, result = visualize_cam(mask, img_tensor)\n",
    "\n",
    "    heatmap = np.clip(heatmap.squeeze().numpy(), 0, 1)\n",
    "    heatmap_image = Image.fromarray((heatmap * 255).astype('uint8'))\n",
    "\n",
    "    return results, image, heatmap_image\n",
    "\n",
    "iface = gr.Interface(fn=predict, inputs=\"image\", outputs=[\"json\", \"image\", \"image\"],\n",
    "                     description=\"Predict Nucleus Shape and visualize using Grad-CAM.\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
