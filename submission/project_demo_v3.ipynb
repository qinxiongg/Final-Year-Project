{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7878\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tanqi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from attribute_predictor import AttributePredictor\n",
    "from attribute_predictor_SEBlocks import AttributePredictor_SEB\n",
    "from gradcam.utils import visualize_cam\n",
    "\n",
    "# Functions for load and running model on test dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_path, checkpoint_name):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.base_path = base_path\n",
    "        self.checkpoint_name = checkpoint_name\n",
    "\n",
    "        # Define a mapping of labels to indices\n",
    "        self.label_mapping = {\n",
    "            \"irregular\": 0,\n",
    "            \"segmented-bilobed\": 1,\n",
    "            \"segmented-multilobed\": 2,\n",
    "            \"unsegmented-band\": 3,\n",
    "            \"unsegmented-indented\": 4,\n",
    "            \"unsegmented-round\": 5\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.base_path, self.data_frame.iloc[idx, -1])\n",
    "        label_name = self.data_frame.iloc[idx, 4]\n",
    "        label = self.label_mapping[label_name]  # Convert label names to indices\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        image = image_preprocessing(image,checkpoint_name=self.checkpoint_name)\n",
    "        image = transform(image).to(device)\n",
    "\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def evaluate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)  # outputs should be a list of tensors\n",
    "\n",
    "            # If you're evaluating a specific attribute, select the correct tensor\n",
    "            # For instance, if the first tensor corresponds to `nucleus_shape`\n",
    "            outputs_for_attribute = outputs[0]  # Adjust index based on your model's output\n",
    "\n",
    "            _, predicted = torch.max(outputs_for_attribute, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Load and prepare the model\n",
    "def get_image_encoder(pretrained=True):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    \n",
    "    # Infer the output size of the image encoder\n",
    "    with torch.inference_mode():\n",
    "        out = model(torch.randn(5, 3, 224, 224))\n",
    "    assert out.dim() == 2\n",
    "    assert out.size(0) == 5\n",
    "    image_encoder_output_dim = out.size(1)\n",
    "    \n",
    "    return model, image_encoder_output_dim\n",
    "\n",
    "class GradCAMWrapper(torch.nn.Module):\n",
    "    def __init__(self, model, output_index=0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.output_index = output_index\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)[self.output_index]\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device)\n",
    "    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device)\n",
    "    return tensor * std[:, None, None] + mean[:, None, None]\n",
    "\n",
    "def load_model(checkpoint_path):\n",
    "    image_encoder, image_encoder_output_dim = get_image_encoder(pretrained=True)\n",
    "    attribute_sizes = [6]\n",
    "    \n",
    "    if checkpoint_path == \"./log/best_model_SEB.pth\":\n",
    "        model = AttributePredictor_SEB(attribute_sizes,image_encoder_output_dim, image_encoder)\n",
    "    else:\n",
    "        model = AttributePredictor(attribute_sizes, image_encoder_output_dim, image_encoder)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def image_preprocessing(original_image, checkpoint_name):\n",
    "    \n",
    "    if checkpoint_name == \"Segmented nucleus model\":\n",
    "        #convert PIL image to opencv format\n",
    "        image = np.array(original_image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Convert to HSV color space and split channels\n",
    "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv_image)\n",
    "\n",
    "        # Split RGB channels\n",
    "        b, g, r = cv2.split(image)\n",
    "\n",
    "        # Subtract the S channel with the G channel\n",
    "        subtracted_image = cv2.subtract(s, g)\n",
    "        \n",
    "        # Threshold the subtracted image\n",
    "        _, thresh = cv2.threshold(subtracted_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Dilate the thresholded image \n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        dilated_thresh = cv2.dilate(thresh, kernel, iterations = 1)\n",
    "\n",
    "        # Convert the binary threshold image to 3 channels\n",
    "        thresh_3_channel = cv2.merge([dilated_thresh, dilated_thresh, dilated_thresh])\n",
    "\n",
    "        # Element-wise multiplication of the binary threshold with the original image\n",
    "        segmented_image = cv2.multiply(image, thresh_3_channel, scale=1/255)\n",
    "\n",
    "        # convert to BGR format\n",
    "        segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2RGB)\n",
    "        # convert to PIL format\n",
    "        processed_img = Image.fromarray(segmented_image)\n",
    "\n",
    "        return processed_img\n",
    "    \n",
    "    elif checkpoint_name == \"Nucleus crop model\":\n",
    "\n",
    "        min_size=(150, 150)\n",
    "        #convert PIL image to opencv format\n",
    "        image = np.array(original_image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "        # Convert to image to HSV color space and split the channels\n",
    "        HSV_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        H, S, V = cv2.split(HSV_image)\n",
    "\n",
    "        # Split BGR channels \n",
    "        B, G, R = cv2.split(image)\n",
    "\n",
    "        # Subtract the S channel with the G channel\n",
    "        subtracted_image = cv2.subtract(S, G)\n",
    "        \n",
    "        # Threshold the subtracted image\n",
    "        ret, thresholded_image = cv2.threshold(subtracted_image, 0, 255, cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Dilate the thresholded image to improve contour detection\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        dilated_threshold_image = cv2.dilate(thresholded_image, kernel, iterations = 1)\n",
    "\n",
    "        # Find contours\n",
    "        contours, hierarchy = cv2.findContours(dilated_threshold_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Assuming the largest contour is the nucleus, if not empty\n",
    "        if contours:\n",
    "    \n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            original_x, original_y, original_w, original_h = cv2.boundingRect(largest_contour)\n",
    "            \n",
    "            # Calculate the center of the original bounding box\n",
    "            original_center_x = original_x + original_w // 2\n",
    "            original_center_y = original_y + original_h // 2\n",
    "\n",
    "            # Enforce minimum size, ensuring it's centered around the original bounding box\n",
    "            w = max(original_w, min_size[0])\n",
    "            h = max(original_h, min_size[1])\n",
    "\n",
    "            # Adjust x and y to crop the image around the center of the bounding box\n",
    "            new_x = max(original_center_x - w // 2, 0)\n",
    "            new_y = max(original_center_y - h // 2, 0)\n",
    "\n",
    "            # Adjust the end points, making sure we don't go out of the image boundaries\n",
    "            new_x_end = min(new_x + w, image.shape[1])\n",
    "            new_y_end = min(new_y + h, image.shape[0])\n",
    "\n",
    "            # Correct the coordinates if they go out of bounds\n",
    "            if new_x_end > image.shape[1]:\n",
    "                new_x = image.shape[1] - w\n",
    "            if new_y_end > image.shape[0]:\n",
    "                new_y = image.shape[0] - h\n",
    "\n",
    "            # Crop the image with the adjusted coordinates\n",
    "            cropped_nucleus = image[new_y:new_y_end, new_x:new_x_end]\n",
    "        \n",
    "            # convert back to RGB format for conversion back to PIL\n",
    "            cropped_nucleus = cv2.cvtColor(cropped_nucleus, cv2.COLOR_BGR2RGB)\n",
    "            # convert to PIL format\n",
    "            processed_image = Image.fromarray(cropped_nucleus)\n",
    "        return processed_image\n",
    "    \n",
    "    else:\n",
    "        return original_image\n",
    "    \n",
    "\n",
    "\n",
    "# Gradio function to handle image input, model prediction, and visualization\n",
    "def predict_and_visualize(original_image, checkpoint_name):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    obj = None\n",
    "    gc.collect()\n",
    "    \n",
    "    original_image = image_preprocessing(original_image, checkpoint_name)\n",
    "    img = transform(original_image).unsqueeze(0).to(device)\n",
    "    checkpoint_path = checkpoint_path_all[checkpoint_name]  \n",
    "    model = load_model(checkpoint_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create the dataset and DataLoader\n",
    "    test_dataset = TestDataset(csv_file=csv_file, base_path=base_path, checkpoint_name=checkpoint_name)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = evaluate_accuracy(model, test_loader, device)\n",
    "    # print(f'Accuracy of the model on the test images: {accuracy:.5f}%')\n",
    "    accuracy = '%.5f'%accuracy\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img)\n",
    "\n",
    "    # Handle output if it's a list\n",
    "    if isinstance(predictions, list):\n",
    "        # Assuming the logits are the first element of the list\n",
    "        logits = predictions[0]\n",
    "    else:\n",
    "        logits = predictions\n",
    "\n",
    "    probabilities = F.softmax(logits.squeeze(), dim=0)\n",
    "    probabilities = probabilities.cpu().numpy()\n",
    "\n",
    "    attribute_values = [\"irregular\", \"segmented-bilobed\", \"segmented-multilobed\", \"unsegmented-band\", \"unsegmented-indented\", \"unsegmented-round\"]\n",
    "    results = {attribute_values[i]: float(probabilities[i]) for i in range(len(attribute_values))}\n",
    "\n",
    "\n",
    "    # Grad-CAM setup\n",
    "    target_layer = model.image_encoder.layer4[-1]\n",
    "    gradcam_model_wrapper = GradCAMWrapper(model, output_index=0)\n",
    "    gradcam = GradCAM(gradcam_model_wrapper, target_layer)\n",
    "    # gradcam_pp = GradCAMpp(gradcam_model_wrapper, target_layer)\n",
    "\n",
    "    mask, _ = gradcam(img)\n",
    "    heatmap, result = visualize_cam(mask, img)\n",
    "    if heatmap.ndim == 3 and heatmap.shape[0] == 3:\n",
    "        heatmap = heatmap[0]\n",
    "        \n",
    "    img_denorm = denormalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "\n",
    "    heatmap_norm = (heatmap.squeeze().cpu() - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "    colored_heatmap = plt.cm.jet(heatmap_norm.numpy())  # This applies the 'jet' colormap\n",
    "\n",
    "    # Convert colored heatmap to an image (discard the alpha channel)\n",
    "    heatmap_img = Image.fromarray((colored_heatmap[:, :, :3] * 255).astype(np.uint8))\n",
    "\n",
    "    # Combine the heatmap with the original image\n",
    "    img_pil = to_pil_image(img_denorm.squeeze()).convert(\"RGB\")\n",
    "    heatmap_on_image = Image.blend(img_pil, heatmap_img, alpha=0.4)\n",
    "\n",
    "    return original_image, heatmap_on_image, results, accuracy\n",
    "    # return original_image, heatmap_img, heatmap_on_image, results \n",
    "\n",
    "checkpoint_path_all = {\n",
    "    \"Default model\" : \"./log/best_model_nucleus.pth\",\n",
    "    \"Nucleus crop model\" : \"./log/best_model_nucleus_crop.pth\",\n",
    "    \"Segmented nucleus model\" : \"./log/best_model_segmented.pth\",\n",
    "    \"Squeeze-And-Excitation model\" : \"./log/best_model_SEB.pth\"\n",
    "}\n",
    "\n",
    "# Transform the input image to match the model's expected input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "csv_file='pbc_attr_v1_val.csv'\n",
    "base_path='./data/PBC/'\n",
    "\n",
    "sample_images = [\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/neutrophil/BNE_810657.jpg\"],\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/neutrophil/BNE_681139.jpg\"],\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/neutrophil/BNE_330256.jpg\"],\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/monocyte/MO_574699.jpg\"],\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/neutrophil/SNE_153895.jpg\"],\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/neutrophil/BNE_961533.jpg\"],\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/monocyte/MO_139718.jpg\"],\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/lymphocyte/LY_654739.jpg\"],\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/lymphocyte/LY_623093.jpg\"],\n",
    "    [\"./data/PBC/PBC_dataset_normal_DIB/neutrophil/SNE_799574.jpg\"]\n",
    "]\n",
    "\n",
    "_HEADER_ = '''\n",
    "<h2>Final Year Project: Cell Image Staining and Analysis using Robust AI</h2>\n",
    "'''\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(_HEADER_)\n",
    "    with gr.Row(variant=\"panel\"):\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"pil\", label=\"Sample image for Grad-CAM visualization\")\n",
    "            model_dropdown = gr.Dropdown(choices=checkpoint_path_all, label=\"Select Model\")\n",
    "            submit_button = gr.Button()\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                output_image = gr.Image(type=\"pil\", label=\"Input Image\", width=224, height=224)\n",
    "                output_image_heatmap = gr.Image(type=\"pil\", label=\"Result on Image\", width=224, height=224)\n",
    "            with gr.Row():\n",
    "                output_label = gr.Label(num_top_classes=6, label=\"Nucleus Shape Prediction\")\n",
    "            with gr.Row():\n",
    "                output_accuracy = gr.Label(label=\"Accuracy (%) of the model on the test dataset:\")\n",
    "    \n",
    "    submit_button.click(predict_and_visualize, inputs=[image_input, model_dropdown], outputs=[output_image, output_image_heatmap, output_label, output_accuracy])\n",
    "                         \n",
    "    with gr.Row(variant=\"panel\"):\n",
    "        gr.Examples(\n",
    "                    examples=sample_images,\n",
    "                    inputs=[image_input],\n",
    "                    label=\"Examples\",\n",
    "                )    \n",
    "          \n",
    "    description=\"Choose a model from the dropdown box. Select a test image to visualize the model's Gradient Class Activation Map. Then run click on submit to start the test phrase and Grad-CAM visualization.\",               \n",
    "demo.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
